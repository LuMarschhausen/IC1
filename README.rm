# Respostas e minhas Interpretações acerca dos resultados.

- Questão 1
Média de iterações para convergir: 10.063
Média de divergência P[f(x) ≠ g(x)]: 0.109334
**Resposta b aprox 15** 
Esta questão ilustra conceitos fundamentais de aprendizado de máquina, como a capacidade de um algoritmo de aprender uma função
a partir de exemplos rotulados (aprendizado supervisionado), o processo de ajuste iterativo dos parâmetros do modelo (aprendizado online) 
e a avaliação de desempenho do modelo em dados não vistos (generalização). Além disso, destaca a importância de avaliar a eficiência e a eficácia 
dos algoritmos de aprendizado, tanto em termos de número de iterações para convergência quanto em termos de precisão da classificação 
em novos dados.

- Questão 2
Média de divergência P[f(x) ≠ g(x)]: 0.1095244
**Resposta c aprox 0.1**
Esta questão está diretamente relacionada aos conceitos de generalização e capacidade de aprendizado. Aprendemos que em aprendizado de máquina, 
a capacidade de um modelo de generalizar para novos dados fora do conjunto de treinamento é crucial. A divergencia pedida, quantifica o desempenho 
do modelo em dados não vistos, permitindo avaliar se o Perceptron, apesar de ser um algoritmo simples, consegue aprender uma boa aproximação da 
função target a partir de um pequeno conjunto de dados.

- Questão 3
Média de iterações para convergir: 118.22
**Resposta b aprox 100**
Esta questão está relacionada aos conceitos de eficiência do algoritmo e complexidade computacional. Em aprendizado de máquina, a eficiência 
com a qual um algoritmo converge para uma solução é crucial, especialmente com conjuntos de dados maiores. O número de iterações para a convergência 
do PLA reflete a rapidez com que o algoritmo ajusta seus pesos para classificar corretamente todos os pontos de treinamento. 

- Questão 4
Média de divergência P[f(x) ≠ g(x)]: 0.013131900000000002
**Resposta b aprox 0.01**
Esta questão relaciona-se diretamente com os conceitos de overfitting, underfitting, e capacidade de generalização em aprendizado de máquina. 
Ao aumentar o número de pontos de treinamento, o modelo obtido pelo Perceptron tende a capturar melhor a verdadeira distribuição dos dados, 
reduzindo a divergência entre a função target f e hipótege g. Isso mostra que, com mais dados, o modelo é capaz de generalizar melhor, reduzindo 
o risco de overfitting e underfitting.

- Questão 5
Sim, À medida que o número de pontos de treinamento N aumenta, é de se esperar que a probabilidade de divergência entre a função target f e a hipótese 
g encontrada pelo Perceptron, diminua. Isso ocorre porque um conjunto de treinamento maior proporciona mais informações para o algoritmo de 
aprendizado, resultando em uma hipótese mais precisa e com menor erro de generalização. Porém, o aumento em N também pode levar a um aumento 
no número médio de iterações necessárias para a convergência do Perceptron, devido à complexidade adicional dos dados. Portanto, há um trade-off 
entre a capacidade de generalização do modelo e a eficiência computacional do algoritmo de aprendizado, logo, aumentando a importância de escolher 
um tamanho adequado para o conjunto de treinamento em problemas de aprendizado de máquina.

# Regressão linear

- Questão 1
Média de E_in: 0.039180000000000006
**Resposta c aprox 0.01**

- Questão 2 
Média de E_out: 0.04808300000000001
**Resposta c aprox 0.01**
A métrica E_out reflete a capacidade do modelo de generalizar para novos dados que não foram vistos durante o treinamento. Em aprendizado de 
máquina, um modelo bem ajustado deve apresentar um E_out baixo, indicando que ele consegue manter um bom desempenho em dados fora da amostra. 
Comparar E_in com E_out​ ajuda a identificar se o modelo está sofrendo de overfitting, onde ele performa bem nos dados de treinamento (E_in ​
baixo) mas mal nos dados de teste (E_out alto). A proximidade entre E_in e E_out neste experimento sugere que a Regressão Linear conseguiu 
aprender uma hipótese que generaliza bem, demonstrando sua eficácia como técnica de aprendizado de máquina para problemas de classificação linear.







